{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "df = pd.read_csv('podcastdata_dataset.csv')\n",
    "def preprocess_tolower(text):\n",
    "    text=text.lower()\n",
    "    return text\n",
    "\n",
    "df['text']=df['text'].apply(preprocess_tolower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    # SemanticSplitterNodeParser,\n",
    ")\n",
    "from llama_index.core import Document \n",
    "# from llama_index.core.node_parser import SimpleNodeParser\n",
    "# from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import LangchainNodeParser\n",
    "# from llama_index.core.schema import MetadataMode \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        text=row['text'],\n",
    "        metadata={\n",
    "            'Title': row['title'],\n",
    "            'Guest': row['guest'],\n",
    "            'id': row['id']\n",
    "        }\n",
    "    )\n",
    "    for _, row in df[:3].iterrows()\n",
    "]\n",
    "\n",
    "parser = LangchainNodeParser(RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap = 200\n",
    "    )\n",
    ")\n",
    "nodes = parser.get_nodes_from_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\HYDE\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "test_emeds = embed_model.get_text_embedding(\"Hello World!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sai18\\AppData\\Local\\Temp\\ipykernel_21060\\2198921486.py:2: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  ctx_base = ServiceContext.from_defaults(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import ServiceContext, set_global_service_context\n",
    "ctx_base = ServiceContext.from_defaults(\n",
    "    llm=None,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.33s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.92s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.74s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.49s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:07<00:00,  7.17s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:10<00:00, 10.09s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.53s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.83s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.56s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.00s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.86s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:05<00:00,  5.20s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.77s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "sentence_index = VectorStoreIndex(nodes, service_context=ctx_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x0000028BBFF405D0>\n"
     ]
    }
   ],
   "source": [
    "print(sentence_index)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
