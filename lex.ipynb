{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core import Document \n",
    "from llama_index.core.node_parser import LangchainNodeParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.ingestion import IngestionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('podcastdata_dataset.csv')\n",
    "def preprocess_tolower(text):\n",
    "    text=text.lower()\n",
    "    return text\n",
    "\n",
    "df['text']=df['text'].apply(preprocess_tolower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(\n",
    "        text=row['text'],\n",
    "        metadata={\n",
    "            'Title': row['title'],\n",
    "            'Guest': row['guest'],\n",
    "            'id': row['id']\n",
    "        }\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "parser = LangchainNodeParser(RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap = 200\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads BAAI/bge-small-en-v1.5\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\n",
    "                                                name=\"transcripts_db\",\n",
    "                                                metadata={\"hnsw:space\": \"cosine\"}\n",
    "                                            )\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "transformations=[parser, Settings.embed_model]\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "                transformations=transformations,\n",
    "                vector_store=vector_store\n",
    "            )   \n",
    "            \n",
    "# Ingest directly into a vector db\n",
    "pipeline.run(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pipeline\n",
    "pipeline.persist(\"./pipeline_storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations=[parser, Settings.embed_model]\n",
    "\n",
    "# load and restore state\n",
    "new_pipeline = IngestionPipeline(\n",
    "    transformations=transformations\n",
    ")\n",
    "new_pipeline.load(\"./pipeline_storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will run instantly due to the cache\n",
    "nodes = new_pipeline.run(documents=docs)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_collection.query(\n",
    "    query_texts=[\n",
    "        '''\n",
    "        Lex Fridman (/ˈfriːdmən/; born 15 August 1983)[2] is a Russian-American computer scientist and podcaster. Since 2018 he has hosted the Lex Fridman Podcast, where he interviews notable figures from various fields such as science, technology, sports, and politics.\n",
    "        Fridman rose to prominence in 2019 after Elon Musk praised his study which concluded that drivers remained focused while using Tesla's semi-autonomous driving system. The study was criticized by AI experts and was not peer-reviewed.\n",
    "        ''',\n",
    "        '''\n",
    "        Fridman was born in the Soviet Union and grew up in Moscow.[3] He is of Jewish descent.[5] His father Alexander Fridman is a plasma physicist and professor at Drexel University. His brother Gregory was also a professor at Drexel.\n",
    "        '''\n",
    "    ],\n",
    "    n_results=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
